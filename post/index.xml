<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Open Neuromorphic</title><link>https://open-neuromorphic.org/post/</link><description>Recent content in Posts on Open Neuromorphic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 02 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://open-neuromorphic.org/post/index.xml" rel="self" type="application/rss+xml"/><item><title>SNN library benchmarks</title><link>https://open-neuromorphic.org/p/snn-library-benchmarks/</link><pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate><guid>https://open-neuromorphic.org/p/snn-library-benchmarks/</guid><description>&lt;img src="https://open-neuromorphic.org/p/snn-library-benchmarks/framework-benchmarking-16k-header.png" alt="Featured image of post SNN library benchmarks" />SNN library benchmarks Open Neuromorphic&amp;rsquo;s list of SNN frameworks currently counts 10 libraries, and those are only the most popular ones! As the sizes of spiking neural network models grow thanks to deep learning, optimization becomes more important for researchers and practitioners alike. Training SNNs is often slow, as the stateful networks are typically fed sequential inputs. Today&amp;rsquo;s most popular training method then is some form of backpropagation through time, whose time complexity scales with the number of time steps.</description></item><item><title>Bits of Chips | TrueNorth</title><link>https://open-neuromorphic.org/p/bits-of-chips-truenorth/</link><pubDate>Mon, 27 Mar 2023 00:00:00 +0000</pubDate><guid>https://open-neuromorphic.org/p/bits-of-chips-truenorth/</guid><description>&lt;img src="https://open-neuromorphic.org/p/bits-of-chips-truenorth/brain-to-chip.png" alt="Featured image of post Bits of Chips | TrueNorth" />Why do we want to emulate the brain? If you have ever read an article on neuromorphic computing, you might have noticed that in the introduction of each of these there is the same statement: &amp;ldquo;The brain is much powerful than any AI machine when it comes to cognitive tasks but it runs on a 10W power budget!&amp;rdquo;. This is absolutely true: neurons in the brain communicate among each other by means of spikes, which are short voltage pulses that propagate from one neuron to the other.</description></item><item><title>Efficient compression for event-based data</title><link>https://open-neuromorphic.org/p/efficient-compression-for-event-based-data/</link><pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate><guid>https://open-neuromorphic.org/p/efficient-compression-for-event-based-data/</guid><description>&lt;img src="https://open-neuromorphic.org/p/efficient-compression-for-event-based-data/file_read_benchmark.png" alt="Featured image of post Efficient compression for event-based data" />Efficient compression for event-based data Datasets grow larger in size As neuromorphic algorithms tackle more complex tasks that are linked to bigger datasets, and event cameras mature to have higher spatial resolution, it is worth looking at how to encode that data efficiently when storing it on disk. To give you an example, Prophesee&amp;rsquo;s latest automotive object detection dataset is some 3.5 TB in size for under 40h of recordings with a single camera.</description></item><item><title>Digital neuromophic hardware read list</title><link>https://open-neuromorphic.org/p/digital-neuromophic-hardware-read-list/</link><pubDate>Wed, 11 Jan 2023 00:00:00 +0000</pubDate><guid>https://open-neuromorphic.org/p/digital-neuromophic-hardware-read-list/</guid><description>&lt;img src="https://open-neuromorphic.org/p/digital-neuromophic-hardware-read-list/frenkel-thesis.png" alt="Featured image of post Digital neuromophic hardware read list" />Here&amp;rsquo;s a list of articles and theses related to digital hardware designs for neuomorphic applications. I plan to update it regularly. To be redirected directly to the sources, click on the titles!
If you are new to neuromorphic computing, I strongly suggest to get a grasp of how an SNN works from this paper. Otherwise, it will be pretty difficult to understand the content of the papers listed here.
2015 TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip, Filipp Akopyan et al.</description></item><item><title>Spiking neurons: a digital hardware implementation</title><link>https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/</link><pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate><guid>https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/</guid><description>&lt;img src="https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/loihi.png" alt="Featured image of post Spiking neurons: a digital hardware implementation" />Spiking neurons In this article, we will try to model a layer of Leaky Integrate and Fire (LIF) spiking neurons using digital hardware: registers, memories, adders and so on. To do so, we will consider a single output neuron connected to multiple input neurons from a previous layer.
In a Spiking Neural Network (SNN), neurons communicate by means of spikes: these activation voltages are then converted to currents through the synapses, charging the membrane potential of the destination neuron.</description></item><item><title>Open Neuromorphic</title><link>https://open-neuromorphic.org/p/open-neuromorphic/</link><pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate><guid>https://open-neuromorphic.org/p/open-neuromorphic/</guid><description>&lt;img src="https://open-neuromorphic.org/p/open-neuromorphic/ONM.png" alt="Featured image of post Open Neuromorphic" />This organisation is created by a loose collective of open source collaborators across academia, industry and individual contributors. What connects us is the love for building tools that can be used in the neuromorphic community and we want to share ownership of this vision.
Open Neuromorphic (ONM) provides the following things:
A curated list of software frameworks to make it easier to find the tool you need. A platform for your code.</description></item></channel></rss>